mp<-mp[mp$STATE!="02"|mp$STATE!="15",]
plot(st_geometry(mp),add = TRUE, reset = FALSE, col = NA, border="grey")
plot(mp)
unique(mp$STATE)
mp<-mp[mp$STATE!="02"&mp$STATE!="15",]
unique(mp$STATE)
plot(mp)
# subseet mp to only contiguous states
# 02 = alaska
# 15 = hawaii
# 72 = puerto rico
mp<-mp[mp$STATE!="02"&mp$STATE!="15"|mp$STATE!="72",]
plot(mp$GEO_ID)
plot(mp)
# subseet mp to only contiguous states
# 02 = alaska
# 15 = hawaii
# 72 = puerto rico
mp<-mp[mp$STATE!="02"&mp$STATE!="15"&mp$STATE!="72",]
plot(st_geometry(mp),add = TRUE, reset = FALSE, col = NA, border="grey")
plot(st_geometry(mp))
# plot to check the map lines up
plot(grb_slice, border = NA, reset = FALSE)
#nc = sf::read_sf(system.file("gpkg/nc.gpkg", package = "sf"), "nc.gpkg")
plot(st_geometry(mp),add = TRUE, reset = FALSE, col = NA, border="red")
# crop the raster file to borders
mp = st_transform(mp, st_crs(grb_slice)) # datum transformation
plot(grb_slice[mp], border = NA, reset=FALSE) #breaks = qu_0_omit(prec_slice[[1]]), reset = FALSE)
plot(grb_slice[mp], border = NA, breaks = qu_0_omit(grb_slice[[1]]), reset = FALSE)
qu_0_omit = function(x, ..., n = 22) {
if (inherits(x, "units"))
x = units::drop_units(na.omit(x))
c(0, quantile(x[x > 0], seq(0, 1, length.out = n)))
}
plot(grb_slice[mp], border = NA, breaks = qu_0_omit(grb_slice[[1]]), reset = FALSE)
plot(grb_slice[mp])
# plot to check the map lines up
plot(grb_slice, border = NA, reset = FALSE)
# plot to check the map lines up
plot(grb_slice[mp], border = NA, reset = FALSE)
grb_slice_subset<-grb_slice[mp]
sf_use_s2(FALSE)
grb_slice_subset<-grb_slice[mp] # error duplicate vertex
plot(grb_slice_subset, border = NA, breaks = qu_0_omit(grb_slice[[1]]), reset = FALSE)
plot(grb_slice_subset, border = NA, breaks = qu_0_omit(grb_slice[[1]]), reset = FALSE, na.rm=TRUE)
plot(grb_slice[mp], border = NA, breaks = qu_0_omit(grb_slice[[1]]), reset = FALSE, na.rm=TRUE)
plot(grb_slice_subset, border = NA, reset = FALSE)
#plot(grb_slice[mp], border = NA, reset = FALSE, na.rm=TRUE)
plot(st_geometry(nc), add = TRUE, reset = FALSE, col = NA, border = 'red')
#plot(grb_slice[mp], border = NA, reset = FALSE, na.rm=TRUE)
plot(st_geometry(mp), add = TRUE, reset = FALSE, col = NA, border = 'red')
plot(grb_slice_subset, border = NA, reset = FALSE) # plot raster data
plot(st_geometry(mp), add = TRUE, reset = FALSE, col = NA, border = 'red') # plot map lines
# check to see if I can isolate single county
mp_1<-mp[mp$COUNTY=="029"]
# check to see if I can isolate single county
mp_1<-mp[mp$COUNTY=="029",]
grb_slice_county<-grb_slice[mp_1]
plot(grb_slice_county, border = NA, reset = FALSE) # plot raster data
plot(st_geometry(mp_1), add = TRUE, reset = FALSE, col = NA, border = 'red') # plot map lines
# check to see if I can isolate single county
mp_1<-mp[mp$GEO_ID=="0500000US01029",]
grb_slice_county<-grb_slice[mp_1]
plot(grb_slice_county, border = NA, reset = FALSE) # plot raster data
plot(st_geometry(mp_1), add = TRUE, reset = FALSE, col = NA, border = 'red') # plot map lines
# summarize raster by county
f.m<-function(x){median(x)} # calculate median value
f.m(grb_slice_county)
grb_slice_county
?st_extract
st_extract(grb_slice_county)
f.m(grb_slice, at=mp_1)
st_extract(grb_slice_county)
st_extract(grb_slice_county, median, at=mp_1)
st_extract(grb_slice_county, at=mp_1)
st_extract(grb_slice, at=mp_1)
aggregate(grb_slice, mp_1, f.m)|>st_as_sf()
# summarize raster by county
f.m<-function(x){mean(x)} # calculate median value
aggregate(grb_slice, mp_1, f.m)|>st_as_sf()
aggregate(grb_slice, mp_1, f.m)
output<-aggregate(grb_slice, mp_1, f.m)|>st_as_sf()
output
# summarize raster by county
f.m<-function(x){median(x)} # calculate median value
output<-aggregate(grb_slice, mp_1, f.m)|>st_as_sf()
output
output$geometry
output$NLDAS_MOS0125_H.A20221027.0600.002.grb
output[[1]]
?aggregate
# grb_slice = band (stars raster) to analyze
# mp_1 = polygon selecting pixels
aggregate_county<-function(grb_slice, mp_1){
require(stats)
require(sf)
require(raster)
output<-aggregate(grb_slice, mp_1, median)|>st_as_sf()
return(output[[1]]) # this returns median value from map subset !!!
}
?lapply
# mp = shape file of counties
# grb = raster file
county_median<-function(mp, grb){
# define county / vector names
counties<-unique(mp$GEO_ID)
#out<-rep(NA, length(counties))
# names(out)<-counties
# select, summarize and return
out<-sapply(counties, select_county, grb, mp, USE.NAMES = TRUE)
return(out)
}
# grb_slice = band (stars raster) to analyze
# mp_1 = polygon selecting pixels
aggregate_county<-function(grb_slice, mp_1){
require(stats)
require(sf)
require(raster)
output<-aggregate(grb_slice, mp_1, median)|>st_as_sf()
return(output[[1]]) # this returns median value from map subset !!!
}
select_county<-function(input, grb_slice, mp){
mp_1<-mp[mp$GEO_ID==input,]
output<-aggregate_county(grb_slice, mp_1)
return(output)
}
# mp = shape file of counties
# grb = raster file
county_median<-function(mp, grb){
# define county / vector names
counties<-unique(mp$GEO_ID)
#out<-rep(NA, length(counties))
# names(out)<-counties
# select, summarize and return
out<-sapply(counties, select_county, grb, mp, USE.NAMES = TRUE)
return(out)
}
counties.m<-county_median(mp, grb_slice)
counties.m
# make map from median values
plot(st_geometry(mp), add = TRUE, reset = FALSE, col = counties.m, border = 'red')
# make map from median values
plot(st_geometry(mp), reset = FALSE, col = counties.m, border = 'red')
# make map from median values
plot(st_geometry(mp), col = counties.m, border = 'red')
# make map from median values
plot(st_geometry(mp), col = counties.m, border = 'white')
# make map from median values
plot(st_geometry(mp), col = counties.m, border = 'white')
# make map from median values
colfun<-colorRampPalette(c("blue", "green", "yellow"))
plot(st_geometry(mp), col = colfun(counties.m), border = 'white')
colfun
colfun(300)
ggplot()+
geom_stars(data=st_geometry(mp))+
coord_equal()+
#facet_wrap(~band)+
theme_void()+
scale_fill_viridis_c()
rlang::last_error()
ggplot()+
geom_stars(data=mp)+
coord_equal()+
#facet_wrap(~band)+
theme_void()+
scale_fill_viridis_c()
ggplot()+
geom_stars(data=st_geometry(mp))+
coord_equal()+
#facet_wrap(~band)+
theme_void()+
scale_fill_viridis_c()
st_geometry(mp)
ggplot()+
geom_stars(data=st_geometry(mp))
mp
st_geometry(mp)
ggplot()+
geom_stars(data=st_geometry(mp))+
coord_equal()+
facet_wrap(~band)+
theme_void()+
scale_fill_viridis_c()
ggplot()+
geom_stars(data=st_geometry(mp))+
coord_equal()+
facet_wrap(~band)+
theme_void()+
scale_fill_viridis_c()
ggplot()+
geom_stars(data=st_geometry(mp),aes(colours=counties.m))+
coord_equal()+
facet_wrap(~band)+
theme_void()+
scale_fill_viridis_c()
?geom_stars
ggplot()+
geom_stars(data=st_geometry(mp),aes(colours=counties.m), sf=TRUE)+
coord_equal()+
facet_wrap(~band)+
theme_void()+
scale_fill_viridis_c()
?st_geometry
?geom_stars
ggplot()+
geom_stars(data=st_geometry(mp),aes(fill=counties.m), sf=TRUE)+
coord_equal()+
facet_wrap(~band)+
theme_void()+
scale_fill_viridis_c()
ggplot()+
geom_stars(data=st_geometry(mp),aes(fill=counties.m))+
coord_equal()+
facet_wrap(~band)+
theme_void()+
scale_fill_viridis_c()
geom_stars
ggplot(st_geometry(mp))+
geom_stars(aes(fill=counties.m))+
coord_equal()+
facet_wrap(~band)+
theme_void()+
scale_fill_viridis_c()
print_stars(mp)
library(stars)
print_stars(mp)
?print.stars
print.stars(mp)
print(mp)
counties.m
grb
names(grb)
# set path
path<-"/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/"
file<-list.files("path")
file
path
?list.files
file<-list.files(path=path)
file
file[str_match(file, pattern="*grb")]
?regex
file[str_match(file, pattern="*.grb")]
file[str_match(file, pattern="NLDAS*.grb")]
str_match(file, pattern="NLDAS*.grb")
file[str_match(file, pattern="\\.grb$")]
str_match(file, pattern="\\.grb$")
file<-list.files(path=path, pattern="\\.grb$")
file
mpfile<-list.files(path=path, pattern="\\.json$")
mpfile
?mclapply
# function extracts slice and wraps county median calculations
# mp = stars polygons object
# grb = stars grb stack
# slice = band to select from stack
grab_calculate<-function(slice, mp, grb){
# select slice
grb_slice<-slice(grb, index=slice, along="band") # select band/layer to subset
# calculate median values for slice
output<-county_median(mp, grb_slice)
return(output) # return results
}
library(parallel)
# grb_slice = band (stars raster) to analyze
# mp_1 = polygon selecting pixels
aggregate_county<-function(grb_slice, mp_1){
require(stats)
require(sf)
require(raster)
output<-aggregate(grb_slice, mp_1, median)|>st_as_sf()
return(output[[1]]) # this returns median value from map subset !!!
}
select_county<-function(input, grb_slice, mp){
mp_1<-mp[mp$GEO_ID==input,]
output<-aggregate_county(grb_slice, mp_1)
return(output)
}
# mp = shape file of counties
# grb = raster file
county_median<-function(mp, grb){
# define county / vector names
counties<-unique(mp$GEO_ID)
#out<-rep(NA, length(counties))
# names(out)<-counties
# select, summarize and return
out<-sapply(counties, select_county, grb, mp, USE.NAMES = TRUE)
return(out)
}
# function extracts slice and wraps county median calculations
# mp = stars polygons object
# grb = stars grb stack
# slice = band to select from stack
grab_calculate<-function(slice, mp, grb){
# select slice
grb_slice<-slice(grb, index=slice, along="band") # select band/layer to subset
# calculate median values for slice
output<-county_median(mp, grb_slice)
return(output) # return results
}
# import files ####
path<-"/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/"
grbfile<-list.files(path=path, pattern="\\.grb$")
grb.stars<-read_stars(paste(path, grbfile, sep="")) # import raster stack
mpfile<-list.files(path=path, pattern="\\.json$")
mp=sf::read_sf(paste(path, mpfile, sep="")) # import map geometry
# select counties for map geometry:
# subseet mp to only contiguous states
# 02 = alaska
# 15 = hawaii
# 72 = puerto rico
mp<-mp[mp$STATE!="02"&mp$STATE!="15"&mp$STATE!="72",]
# crop the raster file to borders
mp = st_transform(mp, st_crs(grb.stars)) # datum transformation
sf_use_s2(FALSE) # turn off s2 (curve projection)
grb_subset<-grb.stars[mp] # subset raster to map area
# define vector of slices:
# band names:
names(grb)
# 20: soil temperature
# 21: soil moisture content kg/m2 0-10 cm
# 22: soil moisture content kg/m2 10-40 cm
# 26: soil moisture availability 0-40 cm
# 29: canopy water evaporation
# 33: plant canopy surface water
# 35: canopy conductance
# 36: leaf area index
slices<-c(20, 21, 22)
# median calculationg for one band take about 50 min on 2018 mac mini with 6 3.2 ghz cores
# increase n cores if necessary
county_median_values<-mclapply(slices, grab_calculate, mp, grb_subset, mc.cores=2, mc.cleanup = TRUE)
names(county_median_values)<-c("Temp_C", "moisture.0.10.kg.m", "moisture.10.40.kg.m")
county_median_values$Temp_C
Sys.time()
substr(Sys.time(), 1, 10)
paste(substr(Sys.time(), 1, 10), collapse="-")
gsub("-", "", substr(Sys.time(), 1, 10))
data.frame(county_median_values$Temp_C)
SoilT<-data.frame(county_median_values$Temp_C)
mp
date<-gsub("-", "", substr(Sys.time(), 1, 10)) # makes date for labeling dataset
date
names(SoilT)<-c("GEO_ID", date)
names(SoilT)<-c(date)
SoilT
SoilT<-data.frame("GEO_ID"=names(county_median_values$Temp_C), county_median_values$Temp_C)
SoilT<-data.frame("GEO_ID"=names(county_median_values$Temp_C), date=county_median_values$Temp_C)
SoilT
SoilT<-data.frame(names(county_median_values$Temp_C), date=county_median_values$Temp_C)
names(SoilT)<-c("GEO_ID", date) # rownames are "GEO_ID"
SoilT
write.csv(SoilT, "/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soiltemp.csv")
SoilM0.10<-data.frame(names(county_median_values$moisture.0.10.kg.m), county_median_values$moisture.0.10.kg.m)
names(SoilM0.10)<-c("GEO_ID", date) # rownames are "GEO_ID"
SoilM10.40<-data.frame(names(county_median_values$moisture.10.40.kg.m), county_median_values$moisture.10.40.kg.m)
names(SoilM10.40)<-c("GEO_ID", date) # rownames are "GEO_ID"
write.csv(SoilM0.10, "/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soilmoisture0_10.csv")
write.csv(SoilM10.40, "/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soilmoisture10_40.csv")
?read.csv
SoilT<-NULL
SoilT<-as.data.frame(read.csv("/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soiltemp.csv", header=TRUE, row.names = 1))
View(SoilT)
SoilM0.10<-as.data.frame(read.csv("/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soilmoisture0_10.csv", header=TRUE, row.names = 1))
SoilM10.40<-as.data.frame(read.csv("/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soilmoisture10_40.csv", header=TRUE, row.names = 1))
?merge
?rbind
as.data.frame(date=county_median_values$Temp_C)
as.data.frame(county_median_values$Temp_C)
SoilT<-base::merge(SoilT, as.data.frame(county_median_values$Temp_C), by="row.names")
SoilT
names(SoilT)[names(SoilT)=="county_median_values$Temp_C"]<-paste("x", date, sep="")
View(SoilT)
SoilT<-SoilT[,-1]
write.csv(SoilT, "/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soiltemp.csv")
SoilT<-as.data.frame(read.csv("/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soiltemp.csv", header=TRUE, row.names = 1))
View(SoilT)
SoilT<-as.data.frame(read.csv("/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soiltemp.csv", header=TRUE, row.names = 1))
row.names(SoilT)<-SoilT$GEO_ID
write.csv(SoilT, "/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soiltemp.csv")
SoilT<-as.data.frame(read.csv("/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soiltemp.csv", header=TRUE, row.names = 1))
View(SoilT)
SoilM0.10<-base::merge(SoilM0.10, as.data.frame(county_median_values$moisture.0.10.kg.m), by="row.names")
names(SoilM0.10)[names(SoilM0.10)=="county_median_values$moisture.0.10.kg.m,"]<-paste("x", date, sep="")
SoilM0.10<-SoilM0.10[,-1]
row.names(SoilM0.10)<-SoilM0.10$GEO_ID
SoilM10.40<-base::merge(SoilM10.40, as.data.frame(county_median_values$moisture.10.40.kg.m), by="row.names")
names(SoilM10.40)[names(SoilM10.40)=="county_median_values$moisture.10.40.kg.m"]<-paste("x", date, sep="")
SoilM10.40<-SoilM10.40[,-1]
row.names(SoilM10.40)<-SoilM10.40$GEO_ID
write.csv(SoilM0.10, "/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soilmoisture0_10.csv")
write.csv(SoilM10.40, "/Users/dietrich/Documents/GitHub/MycoCast/Map_Scripts_Test/soilmoisture10_40.csv")
QSeq
library(QSeq)
library(Qseq)
library(devtools)
install_github("djeppschmidt/QSeq")
library(phyloseq)
?mapply
getwd()
itree<-readRDS("Documents/GitHub/QSeq/Test/itree_phyloseq.rds")
setwd("Documents/GitHub/QSeq")
View(as.data.frame(sample_data(itree)))
itreeQ<-QSeq(itree, "eub_gene_copy")
QSeq<-function(ps, abundance){
# check to make sure column exists for abundance data
if(!any(names(phyloseq::sample_data(ps))==abundance)){
stop("Error: abundance column does not exist in sample data. Metadata has the following columns: ", names(phyloseq::sample_data(ps)), " ; You asked for: ", abundance)
}
ps<-phyloseq::prune_samples(!is.na(sample_data(ps)[[abundance]]), ps)
print(paste(sum(is.na(phyloseq::sample_data(ps)[[abundance]])), " sample(s) missing abundance abundance data. Removing ", sum(is.na(phyloseq::sample_data(ps)[[abundance]])), " samples."))
# test to see if works []
scale<-as.numeric(as.character(phyloseq::sample_data(ps)[[abundance]])) # make sure scaling data is not a factor
out<-Qscale(ps, scale)
out
}
#' Function that scales otu table by column in metadata
#'
#' Takes a phyloseq object, and vector for scaling samples in otu table. After scaling, rounds taxon count values to the nearest integer, then returns a phyloseq object with scaled community data.
#' @param ps = phyloseq object with community data, and sample data with at least one column for scaling
#' @param scale = vector that is used for scaling samples
#' @keywords QSeq, Quantitative Sequencing, Quantitative Scaling
#' @export
#' @examples
#' QScale()
Qscale<-function(ps, scale){
otu <- as.data.frame(as.matrix(phyloseq::otu_table(phyloseq::transform_sample_counts(ps, function(x) x/sum(x))))) # transform count data into relative abundance, output to data table
if(!taxa_are_rows(ps)){
otu<-as.data.frame(t(as.matrix(phyloseq::otu_table(phyloseq::transform_sample_counts(ps, function(x) x/sum(x))))))
} # ensure taxa are rows
#scaled<-mapply(`*`, otu, scale)
#colnames(scaled)<-sample_names(ps)
#rownames(scaled)<-taxa_names(ps)
#scaled<-round(scaled)
otu[] <- mapply(`*`, otu, scale) # Multiply relative abundance by total abundance (gene)
print(rownames(otu))
scaled<-round(otu) # round taxon abundance to nearest integer
#print(colnames(scaled))
p2<-ps
phyloseq::otu_table(p2) <- phyloseq::otu_table(scaled, taxa_are_rows=T)
return(p2)
}
itreeQ<-QSeq(itree, "eub_gene_copy")
View(as.data.frame(sample_data(itreeQ)))
View(as.data.frame(as.matrix(otu_table(itreeQ))))
View(as.data.frame(as.matrix(otu_table(itree))))
install_github("djeppschmidt/QSeq")
library(QSeq)
itreeQ<-QSeq(itree, "eub_gene_copy")
ot<-data.frame("s1"=c(1,1,1), "s2"=c(1,1,1))
v=c(1,2)
mapply('*', ot, v)
rownames(ot)<-c("a", "b", "c")
mapply('*', ot, v)
ot
mapply('*', ot, v, USE.NAMES = T)
ot[]<-mapply('*', ot, v)
ot
install_github("djeppschmidt/QSeq")
library(QSeq)
itreeQ<-QSeq(itree, "eub_gene_copy")
QSeq
Qscale
library(QSeq)
QSeq::Qscale
library(roxygen2)
getwd()
document()
install_github("djeppschmidt/QSeq")
install_github("djeppschmidt/QSeq", force = TRUE)
library(QSeq)
QSeq::Qscale
QScale
library(QScale)
library(QSeq)
Qscale
getwd()
library(devtools)
document()
document()
getwd()
document()
library(roxygen2)
library(devtools)
document()
install_github("djeppschmidt/QSeq")
library(QSeq)
library(phyloseq)
install_github("djeppschmidt/QSeq")
library(QSeq)
library(phyloseq)
itree<-readRDS("Trash/itree_phyloseq.rds")
itree<-readRDS("Trash/Test/itree_phyloseq.rds")
itree<-readRDS("~/Trash/Test/itree_phyloseq.rds")
itree<-readRDS("Test/itree_phyloseq.rds")
itreeQ<-QSeq(itree, "eub_gene_copy")
install_github("djeppschmidt/QSeq")
library(QSeq)
itreeQ<-QSeq(itree, "eub_gene_copy")
document()
install_github("djeppschmidt/QSeq")
library(QSeq)
install_github("djeppschmidt/QSeq")
library(QSeq)
itreeQ<-QSeq(itree, "eub_gene_copy")
document()
rnorm(800, 30, 10)
rnorm(10, 800, 30)
range(rnorm(10, 800, 30))
100/800
range(rnorm(10, 800, 30))
range(rnorm(10, 800, 30))
